# ğŸ“Š Proyecto Final â€“ Curso de ETL y Ciencia de Datos

## ğŸ‘¥ Participantes:
- Juan JosÃ© Bonilla PinzÃ³n.
- Juan Manuel GarcÃ­a Ortiz.
- Ricardo MuÃ±oz Bocanegra.
- Soren Fabricius Acevedo Azuero.

## ğŸŒ Contexto del Proyecto

### â“ DescripciÃ³n del problema:
En la era digital, el acceso a internet es fundamental para el desarrollo y la difusiÃ³n del conocimiento cientÃ­fico. En Colombia, la cobertura de internet varÃ­a significativamente entre los diferentes departamentos, lo que podrÃ­a afectar la producciÃ³n cientÃ­fica y el reconocimiento de revistas y grupos de investigaciÃ³n. 

Este proyecto tiene como objetivo analizar la relaciÃ³n entre la cobertura de internet y la producciÃ³n cientÃ­fica en los departamentos de Colombia, utilizando datos de acceso a internet, revistas indexadas y grupos de investigaciÃ³n.

### ğŸ¯ JustificaciÃ³n del proyecto:
El acceso a internet es una herramienta clave para la comunicaciÃ³n y el intercambio de informaciÃ³n cientÃ­fica. Sin embargo, la desigualdad en la cobertura de internet puede limitar las oportunidades de los investigadores en regiones con menor acceso. Analizar esta relaciÃ³n permitirÃ¡ identificar Ã¡reas que podrÃ­an beneficiarse de inversiones en infraestructura de internet, promoviendo asÃ­ un desarrollo cientÃ­fico mÃ¡s equitativo en todo el paÃ­s.

## ğŸ“‚ Estructura del Proyecto  

```
uao_etl/
â”‚â”€â”€ ğŸ“„ pipeline.py           # Script principal del pipeline ETL
â”‚â”€â”€ ğŸ“‚ data/                 # Datos del proyecto
â”‚   â”œâ”€â”€ ğŸ“‚ raw/              # Datos en bruto sin procesar
â”‚   â”œâ”€â”€ ğŸ“‚ processed/        # Datos transformados
â”‚â”€â”€ ğŸ“‚ extract/              # MÃ³dulos de extracciÃ³n de datosrevistas y grupos
â”‚â”€â”€ ğŸ“‚ transform/            # MÃ³dulos de transformaciÃ³n de datos
â”‚â”€â”€ ğŸ“‚ load/                 # MÃ³dulos de carga de datos
â”‚â”€â”€ ğŸ“‚ config/               # ConfiguraciÃ³n
â”‚â”€â”€ ğŸ“‚ logs/                # Logs del proceso ETL

```


## ğŸ“Š IdentificaciÃ³n Inicial de Fuentes

| # | ğŸ“Œ Fuente de Datos | ğŸŒ Origen | ğŸ“„ Tipo | ğŸ“¦ Volumen |
|---|---------------|--------|------|---------|
| 1 | Cobertura mÃ³vil por tecnologÃ­a, departamento y municipio | Datos Abiertos Colombia | CSV, API | ~41 MB |
| 2 | Internet fijo: Accesos por tecnologÃ­a y segmento | Datos Abiertos Colombia | CSV, API | ~340 MB |
| 3 | Revistas Indexadas (Ãndice Nacional Publindex 2017-2020) | Datos Abiertos Colombia | CSV, XML | ~2 MB |
| 4 | Grupos de InvestigaciÃ³n Reconocidos | Datos Abiertos Colombia | CSV, API | ~10 MB |

Estos datasets proporcionarÃ¡n la base necesaria para realizar el anÃ¡lisis exploratorio, la transformaciÃ³n y la modelaciÃ³n de los datos. Al combinar la informaciÃ³n sobre la cobertura de internet con los datos de producciÃ³n cientÃ­fica, esperamos identificar correlaciones y patrones que puedan informar polÃ­ticas pÃºblicas y estrategias de desarrollo.

Este proyecto busca relacionar la cobertura de internet en Colombia (mÃ³vil y fija) con la producciÃ³n cientÃ­fica. Para ello, se realiza un proceso de ETL sobre cuatro datasets:

- **Cobertura de internet fijo**: Contiene informaciÃ³n sobre la penetraciÃ³n del servicio de internet fijo en distintos municipios del paÃ­s.
- **Cobertura de internet mÃ³vil**: Presenta datos sobre la disponibilidad de internet mÃ³vil en diferentes regiones.
- **Revistas indexadas**: Incluye informaciÃ³n sobre publicaciones acadÃ©micas reconocidas en Colombia.
- **Grupos de investigaciÃ³n en Colombia**: Base de datos con los grupos de investigaciÃ³n registrados en el paÃ­s.

El propÃ³sito del notebook es limpiar, transformar e integrar estos datos para su anÃ¡lisis.

---

## 1. Carga de Datos
Los datos son cargados desde archivos CSV utilizando la biblioteca `pandas`. Esto permite manipular grandes volÃºmenes de datos de manera eficiente.

```python
import pandas as pd
import numpy as np
from google.colab import files

# Cargamos las librerÃ­as necesarias
uploaded = files.upload()
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)
```

Se realiza una inspecciÃ³n inicial para verificar los tipos de datos, valores faltantes y detectar posibles inconsistencias:

```python
df.dtypes
df.isnull().sum()
df.describe()
```

---

## 2. Transformaciones por Dataset

### a) Cobertura de Internet Fijo

**Objetivo:** Estandarizar la informaciÃ³n geogrÃ¡fica y eliminar datos innecesarios.

- Se identificaron valores faltantes en la columna `COD MUNICIPIO`. Para evitar errores en el anÃ¡lisis, estos valores se completaron basÃ¡ndose en registros similares.
- Se eliminaron columnas irrelevantes como `CABECERA MUNICIPAL`, que no aportaban valor analÃ­tico.
- Se normalizaron los nombres de municipios y departamentos eliminando tildes y espacios innecesarios para evitar inconsistencias al hacer joins con otros datasets.

```python
df_fijo['MUNICIPIO'] = df_fijo['MUNICIPIO'].str.lower().str.replace(r'\s+', ' ', regex=True)
```

### b) Cobertura de Internet MÃ³vil

**Objetivo:** Limpiar y consolidar la informaciÃ³n para reflejar correctamente la cobertura mÃ³vil en cada municipio.

- Se eliminaron registros duplicados para evitar redundancias en el anÃ¡lisis.
- En algunos municipios habÃ­a registros mÃºltiples con diferentes valores de cobertura. Para solucionar esto, se calculÃ³ un promedio de la cobertura por municipio y departamento.

```python
df_movil = df_movil.groupby(['COD_DEPARTAMENTO', 'COD MUNICIPIO']).mean().reset_index()
```

### c) Revistas Indexadas

**Objetivo:** Completar informaciÃ³n faltante en la ubicaciÃ³n de las revistas indexadas.

- Se detectaron registros donde `DEP_REV_IN` contenÃ­a "bogota" pero el cÃ³digo de departamento estaba ausente. Como BogotÃ¡ tiene cÃ³digo `11`, se imputÃ³ este valor para garantizar una correcta asignaciÃ³n geogrÃ¡fica.
- Se normalizaron nombres de departamentos y ciudades para alinearlos con otros datasets.

```python
df_revistas.loc[df_revistas['DEP_REV_IN'].str.contains('bogota', case=False, na=False), 'COD_DEPARTAMENTO'] = 11
```

### d) Grupos de InvestigaciÃ³n

**Objetivo:** Facilitar la integraciÃ³n con otras bases de datos.

- Se estandarizaron los nombres de los departamentos para hacer coincidir esta informaciÃ³n con los otros datasets.
- Se eliminaron columnas no relevantes que no contribuÃ­an a los anÃ¡lisis posteriores.

---

## 3. IntegraciÃ³n de Datos

Se incluyeron dos tablas adicionales al modelo de datos:
- **DimensiÃ³n de ubicaciÃ³n**: Especifica los cÃ³digos de los municipios, los departamentos a los que pertenecen y el nombre de los municipios.
- **RelaciÃ³n con otras tablas**: En las tablas de salida de los cuatro datasets, la columna `COD_MUNICIPIO` se usarÃ¡ para enlazarlas con la tabla de la dimensiÃ³n de municipios, lo que facilitarÃ¡ consultas y anÃ¡lisis detallados.
-**DimensiÃ³n de tiempo**, en la que se especifica el momento en el que ocurrieron los hechos. Se utilizarÃ¡ una combinaciÃ³n de **aÃ±o y trimestre** para establecer las relaciones en el modelo de datos.

## 4. MÃ©joras al cÃ³digo de ETL

### a) Logs

Con el motivo de dejar registros de la ejecuciÃ³n de nuestro podeso de ETL, se agregaron logs. Hicimos uso de la librerÃ­a logging para crear los registros y os para manejar las rutas en las que se leen y grardan.
El script encargado de darle manejo a los logs se encuentra en  **config/logger.py***. El logger guarda los mensajes en un archivo dentro de la carpeta 'logs' con nombre `etl_<dataset_name>.log`. Si el archivo o la carpeta no existen, se crean automÃ¡ticamente.

### b) EstructuaciÃ³n del proyecto en sistema de archivos

En aras de modularizar el proyecto, decidimos separar los componentes del ETL en carpetas distintas, teniendo una carpeta para las Fases de extracciÃ³n, transformaciÃ³n y carga, otra para as configuraciones y las utlilidades generales. A continuacÃ³n se presenta el diagrama del sistma de carpetas y archivos del proyecto:

```text
uao_etl/
â”œâ”€â”€ config/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ extract/
â”œâ”€â”€ load/
â”œâ”€â”€ notebok/
â”œâ”€â”€ transform/
â”œâ”€â”€ utils/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ README_ETL_Windows.md
â”œâ”€â”€ REPORTE_ETL_COMPLETO.pbix
â”œâ”€â”€ pipeline.py
â””â”€â”€ requirements.txt
```

Siendo el archivo **pipeline.py** el encargado de la ejecuciÃ³n de nuestro proceso completo de ETL para los datasets de cobertura de internet, revistas indexadas y grupos de investigaciÃ³n en Colombia.

### c) archivos de configuraciÃ³n

Se dispuso un archivo de configuraciÃ³n en **config/datasets_config.py** de las operaciones que se le realizarÃ¡ a cada dataset asÃ­ como las rutas en las que se encontrarÃ¡n los datos antes y despuÃ©s de las transformaciones que se le realicen.

Para incluir nuevas fuentes de datos cada dataset debe de tener en su configuraciÃ³n:
- input (str): indicando la ruta donde estÃ¡ ubicado el archivo.
- output (str): indicando el nombre de salida del archivo.
- transformations (Dict[func, args]): listado de transformaciones a aplicar filter_by_year_range.
  
AdemÃ¡s, cada transformaciÃ³n debe de tener:
- func (func): funciÃ³n de transformaciÃ³n.
- args (Dict[str, Any]): argumentos de la transformaciÃ³n.

### d) procesamiento en paralelo

Mediante la librerÃ­a concurrent y el mÃ³dulo ThreadPoolExecutor de futures, implementamos el procesamiento en paralelo de cada uno de los datasets. Esto con el motivo de mejorar el timepo de ejecuciÃ³n de nuestro proceso de ETL. En nuestro caso se hace uso de cuatro hilos debido a que tenemos este numero de datasets a procesar, a cada uno de ellos se le asigna un hilo diferente. Las operciones sobre cada dataset se hace de forma secuencial debido a que existen operaciones que tienen dependencia de transforaciones anteriores. LA implementaciÃ³n d esta mejora, se encuentra en el archivo **pipeline.py**

## 5. VisualizaciÃ³n de los datos

Para visualizar si existe alguna relaciÃ³n entre la cobertura de internet en las diferentes ubicaciones del paÃ­s y la producciÃ³n cientÃ­fica, se creÃ³ un reporte en Power BI que nos permita ilutrar las tendencias que tenga nuestros datos despues de someterlos al proceso de ETL. Dentro de este reporte se pueden encontrar pestaÃ±as que ilustran el comportamiento de la cobertura de internet fijo e internet movil, de la cantidad de grupos de investigaciÃ³n y cantidad de revistas indexadas a travÃ©s del tiempo, ademÃ¡s, podemos encontrar una pestaÃ±a adicional en la que se relaciona la cobertura de internet (puntos de acceso a la red) con la cantidad de grupos de investigaciÃ³n y revistas cientÃ­fics indexadas.

Para relacionar nuestras funetes de datos, creamos un modelo semÃ¡ntico en el que se unen nuestros datasets a travÃ©s de dos tablas de dimensiones, que contienen registros sobre ubicaciÃ³n (departamento y municipio) y se une con nuestros datasets a travÃ©s de la llave que creamos para este proposito (que se estructura de la forma: cÃ³digo departamento_cÃ³digo municipio) y una tabla de dimensiÃ³n de tiempo en la que se tienen lso aÃ±os y los trimestres en lso que ocurrieron los registros de las otras tablas. A continuaciÃ³n se presenta una ilustracion de nuestro modelo semÃ¡ntico:

![Modelo semÃ¡ntico](assets/modelo_semantico.jpeg)

TambiÃ©n presentamos algunas capturas de las pestaÃ±as incluidas en nuestro reporte:

- Compoartamiento de la cobertura de internet fijo a travÃ©s del tiempo:
  
![Internet_fijo_tiempo](assets/internet_fijo_tiempo.jpeg)

- Compoartamiento de la cobertura de internet movil a travÃ©s del tiempo:

![Internet_movil_tiempo](assets/Internet_movil_tiempo.jpeg)

- Cantidad de revistas indexadas a travÃ©s del tiempo:

![revistas indexadas_tiempo](assets/revistas_indexadas_tiempo.jpeg)

- Cantidad de grupos de investigaciÃ³n cientÃ­fica a travÃ©s del tiempo:

![grupos investigaciÃ³n_tiempo](assets/grupos_investigacion_tiempo.jpeg)

- Cobertura de internet y su relaciÃ³n con la producciÃ³n cientÃ­fica:

![Internet_prod_cientifica](assets/Internet_prod_cientifica.jpeg)

**AnÃ¡lisis de los resultados**

- Por departamentos la informaciÃ³n obtenida de las bases de datos, podemos concluir que los departamentos principales (BogotÃ¡, Antioquia, Valle del Cauca, AtlÃ¡ntico y Santander) son los que cuentan con mayores (Coberturas fijas, Grupos de investigaciÃ³n y Revistas indexadas) esto alineÃ¡ndose bastante con lo esperado dado que son los departamentos mÃ¡s grande de Colombia.

- Las revistas indexadas que durante los Ãºltimos aÃ±os 2015-2022 presentÃ³ un mayor nÃºmero de resultados fue en parte por Ciencias Sociales, seguido de Humanidades, sin embargo se mantiene una brecha corta entre las demÃ¡s Ã¡reas del conocimiento, lo que posibilita a futuro un maor enfoque en estas Ã¡reas.

- Por parte de la cobertura MÃ³vil, vemos que Comcel es bastante fuerte en coberturas 2G y LTE sin embargo en 2020 se aprecia una gran caÃ­da en este servicio, tendrÃ­amos que validar mÃ¡s a fondo quÃ© sucediÃ³ durante este aÃ±o ya que se reportan cifras similares a 2018, indicando un retroceso de 2 aÃ±os, los demÃ¡s proveedores mantienen una tendencia similar y con poca brecha.

- Cuando realizamos el anÃ¡lisis cruzado de (departamento, grupos de investigaciÃ³n y revistas) podemos notar que BogotÃ¡ lidera con la mayor cantidad de accesos seguido de antioquia y valle del cauca, sin embargo resulta un detalle bastante relevante, que para todos los departamentos se presenta una alta concentraciÃ³n de revistas con calificaciÃ³n â€œCâ€ lo cual indica una oportunidad de mejora en la calidad de las investigaciones en Colombia, cuando revisamos por grupos de investigaciÃ³n tambiÃ©n notamos este mismo comportamiento.

Por lo cual concluimos que a pesar de que se tenga una mayor cobertura de internet fija, tomando los principales departamentos como referentes, no se logra tener una mayor calificaciÃ³n o clasificaciÃ³n de los grupos de investigaciÃ³n y asÃ­ mismo las revistas indexadas, por lo cual se quedan algunas preguntas abiertas para futuras investigaciÃ³n como Â¿Existe la posibilidad de aumentar la cobertura y mejorar la clasificaciÃ³n de las revistas? Â¿Los grupos de investigaciÃ³n terminan siendo afectados por otras variables exÃ³genas a las planteadas en este trabajo? Â¿La cobertura no es una variable determinante para la clasificaciÃ³n de las revistas? 

---

## 6. Consideraciones Finales

### DesafÃ­os Encontrados
- **Inconsistencia en nombres de municipios y departamentos**: Se resolviÃ³ mediante normalizaciÃ³n y limpieza de strings.
- **Valores nulos en informaciÃ³n geogrÃ¡fica**: Se usaron valores predeterminados y reglas de negocio para completarlos.
- **Registros duplicados en cobertura de internet mÃ³vil**: Se consolidaron mediante agregaciÃ³n.

### Futuras Mejoras
- IncorporaciÃ³n de datos de nuevas fuentes sobre producciÃ³n cientÃ­fica.
- Refinamiento de la integraciÃ³n de datos de cobertura mÃ³vil para reflejar tendencias temporales.
- Desarrollo de modelos predictivos basados en estos datos para analizar el impacto de la conectividad en la investigaciÃ³n cientÃ­fica.

---
## Proyecto ETL - EjecuciÃ³n Local (Windows)

Este proyecto ejecuta un pipeline ETL (ExtracciÃ³n, TransformaciÃ³n, Carga) sobre mÃºltiples datasets, generando logs por cada uno y salidas versionadas.

---

### ğŸ› ï¸ Requisitos

- Python 3.8 o superior
- Git (opcional, para clonar)
- Terminal (CMD, PowerShell o VSCode)

---

### ğŸš€ Pasos para ejecutar en local (Windows)

#### 1. Clona o descarga el repositorio
```bash
git clone https://github.com/tu_usuario/tu_repo.git
cd tu_repo
```

#### 2. Crea un entorno virtual
```bash
python -m venv venv
```

#### 3. Activa el entorno virtual
```bash
venv\Scripts\activate
```

#### 4. Instala las dependencias
```bash
pip install -r requirements.txt
```

#### 5. Ejecuta el pipeline ETL
```bash
python pipeline.py
```

---

### ğŸ“ Salidas

- Los archivos transformados se guardan en: `data/processed/`
- Los logs de cada dataset estÃ¡n en: `logs/`

---

### âœ… Ejemplo de salida

```text
Resumen de ejecuciÃ³n ETL:
- internet_fijo: Ã‰xito
- telefonia_movil: Ã‰xito
- revistas_indexadas: Ã‰xito
- grupos_investigacion: Ã‰xito
```

---

### ğŸ§¹ Para desactivar el entorno virtual

```bash
deactivate
```

## ğŸ“œ Licencia
Este proyecto estÃ¡ licenciado bajo la licencia **Creative Commons** (CC BY 4.0). Puedes compartir y adaptar el contenido, siempre que se otorgue el crÃ©dito correspondiente. 

[![Licencia CC BY 4.0](https://licensebuttons.net/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/)
